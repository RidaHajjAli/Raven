# Ollama Configuration
LOCAL_URL="http://localhost:11434/api/chat"
LOCAL_LLM_MODEL_NAME="llama3.2:1b"

# Optional: Alternative models
# LOCAL_LLM_MODEL_NAME="gemma3:270m"
# LOCAL_LLM_MODEL_NAME="qwen3:0.6b"
# LOCAL_LLM_MODEL_NAME="gemma3:1b"
